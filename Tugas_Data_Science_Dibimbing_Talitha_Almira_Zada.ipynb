import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import load_wine

# Load dataset and convert to DataFrame
data = load_wine()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

df.head()

print(pd.Series(data['target']).value_counts())

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(df.drop('target', axis=1))
scaled_features = scaler.transform(df.drop('target',
										axis=1))

df_feat = pd.DataFrame(scaled_features,
					columns=df.columns[:-1])
df_feat.head()

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.datasets import load_wine

# Split the data into training and testing sets with random_state to ensure consistency
X_train, X_test, y_train, y_test = train_test_split(scaled_features, df['target'], test_size=0.20, random_state=42)

# Create and fit KNN model
knn = KNeighborsClassifier(n_neighbors=5)  # k=1 as per your original code
knn.fit(X_train, y_train)

# Make predictions
pred = knn.predict(X_test)

# Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, pred))
print("\nClassification Report:\n", classification_report(y_test, pred))

error_rate = []

for i in range(1, 40):

	knn = KNeighborsClassifier(n_neighbors=i)
	knn.fit(X_train, y_train)
	pred_i = knn.predict(X_test)
	error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10, 6))
plt.plot(range(1, 40), error_rate, color='blue',
		linestyle='dashed', marker='o',
		markerfacecolor='red', markersize=10)

plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
plt.show()

# Loop over a range of k values
for k in range(1, 11):  # You can change the range (1 to 10) for a wider test of k values
    print(f"Evaluating KNN with k={k}")

    # Create and fit KNN model with current k
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    # Make predictions
    pred = knn.predict(X_test)

    # Evaluate the model
    print("Confusion Matrix:\n", confusion_matrix(y_test, pred))
    print("\nClassification Report:\n", classification_report(y_test, pred))
    print("-" * 50)  # Separator for clarity between each k

from sklearn.metrics import f1_score, precision_score, recall_score

# Menghitung precision, recall, dan F1-score
precision = precision_score(y_test, pred, average='weighted') # Using pred instead of y_pred
recall = recall_score(y_test, pred, average='weighted') # Changed y_val to y_test
f1 = f1_score(y_test, pred, average='weighted') # Using pred instead of y_pred

# Menampilkan hasil
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.datasets import load_wine

# Load dataset
wine = load_wine()
df = pd.DataFrame(wine.data, columns=wine.feature_names)
df['target'] = wine.target

# Scaling the features using StandardScaler
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df.drop(columns=['target']))

# Split the data into training and testing sets with random_state to ensure consistency
X_train, X_test, y_train, y_test = train_test_split(scaled_features, df['target'], test_size=0.20, random_state=42)

# Evaluate KNN with multiple values of k using cross-validation
for k in range(1, 11):  # Test for k values from 1 to 10
    print(f"Evaluating KNN with k={k}")

    # Create and fit KNN model with current k
    knn = KNeighborsClassifier(n_neighbors=k)

    # Cross-validation on training data
    cv_scores = cross_val_score(knn, X_train, y_train, cv=5)  # 5-fold cross-validation

    print(f"Cross-validated accuracy for k={k}: {cv_scores.mean():.4f}")
    print("-" * 50)
